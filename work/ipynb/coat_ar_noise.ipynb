{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23137d4f-b478-426c-b815-f1f89da800e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.6366\n",
      "[2/10] Loss=0.1840\n",
      "[3/10] Loss=0.1714\n",
      "[4/10] Loss=0.1701\n",
      "[5/10] Loss=0.1701\n",
      "[6/10] Loss=0.1698\n",
      "[7/10] Loss=0.1698\n",
      "[8/10] Loss=0.1697\n",
      "[9/10] Loss=0.1697\n",
      "[10/10] Loss=0.1697\n",
      "âœ… Exp3-noise005  Acc=0.5000  Macro-F1=0.5042\n",
      "ğŸ“„ Metrics â†’ result_exp3_noise005.json\n",
      "ğŸ“„ Preds   â†’ pred_exp3_noise005.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp3_noise_9x.py  â€•  ì‹¤í—˜êµ° 3\n",
    "  â€¢ Train : 70ê°œ ì›ë³¸ Ã— 9ë°° ì¦ê°•\n",
    "             (Flip + ColorJitter + GaussianNoise Ïƒ)  = 630 ì´ë¯¸ì§€\n",
    "  â€¢ Test  : split42_70-30.json ì˜ ë‚˜ë¨¸ì§€ 30ê°œ ì›ë³¸\n",
    "  â€¢ Ïƒ : 0.05\n",
    "ì¶œë ¥ : result_exp3_noise<Ïƒ>.json / pred_exp3_noise<Ïƒ>.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. ê¸°ë³¸ import\n",
    "import os, json, random, math, warnings, argparse\n",
    "import numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. CLI ì¸ì  \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--sigma\", type=float, default=0.05,\n",
    "                    help=\"Gaussian noise std (e.g. 0.05)\")\n",
    "args, _ = parser.parse_known_args()          \n",
    "SIGMA = args.sigma\n",
    "tag   = f\"noise{SIGMA:.2f}\".replace('.', '')  # e.g. noise005\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ì‹¤í—˜ ê³ ì • íŒŒë¼ë¯¸í„°\n",
    "SEED   = 42\n",
    "CSV    = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG    = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "\n",
    "SPLIT     = \"split42_70-30.json\"                # ë„¤ ì¼€ì´ìŠ¤ ë™ì¼\n",
    "OUT_MET   = f\"result_exp3_{tag}.json\"\n",
    "OUT_PRED  = f\"pred_exp3_{tag}.csv\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. ì‹œë“œ & ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ ê³ ì •\n",
    "def seed_all(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "seed_all(SEED)\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "if mp.get_start_method(allow_none=True) != \"spawn\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. ë°ì´í„°ì…‹ ì •ì˜\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform, label_enc):\n",
    "        self.df   = dataframe.reset_index(drop=True).copy()\n",
    "        self.dir  = IMG\n",
    "        self.tf   = transform\n",
    "        self.df[\"cls\"] = label_enc.transform(self.df[\"weight_class\"])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        return self.tf(img), torch.tensor(row.cls), row.filename\n",
    "\n",
    "# â”€â”€ ë…¸ì´ì¦ˆ ë³€í™˜\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, std: float): self.std = std\n",
    "    def __call__(self, tensor):     return tensor + torch.randn_like(tensor) * self.std\n",
    "    def __repr__(self):             return f\"AddGaussianNoise(std={self.std})\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Transform ì„¤ì •\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(SIGMA),                    # â˜… ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. split 70 / 30 ê³ ì • ë¡œë“œ\n",
    "df_all = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx = json.load(open(SPLIT, \"r\")); train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:                                   # ìµœì´ˆ ì‹¤í–‰ ì‹œë§Œ ìƒì„±\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df_all)), train_size=70, test_size=30,\n",
    "        stratify=df_all[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":train_idx, \"test\":test_idx}, open(SPLIT, \"w\"))\n",
    "\n",
    "train_df, test_df = df_all.iloc[train_idx], df_all.iloc[test_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. DataLoader (9Ã— ì¦ê°•)\n",
    "base_train_ds = ScrapDataset(train_df, train_tf, le)\n",
    "train_ds      = ConcatDataset([base_train_ds]*9)         # 70 Ã— 9 = 630\n",
    "test_ds       = ScrapDataset(test_df,  test_tf,  le)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True , num_workers=0)\n",
    "test_loader  = DataLoader(test_ds , batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8. ëª¨ë¸, Optimizer, Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model('coat_lite_medium',\n",
    "                                     pretrained=True, num_classes=n_cls)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = CoaTMedium(len(le.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps  = len(train_loader) * EPOCHS\n",
    "warmup_steps = len(train_loader)              # 1 epoch warm-up\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / warmup_steps\n",
    "    progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion  = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 9. í•™ìŠµ ë£¨í”„\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for xb, yb, _ in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[{epoch}/{EPOCHS}] Loss={running_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 10. í‰ê°€\n",
    "model.eval(); y_true, y_pred, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, fn in test_loader:\n",
    "        preds = model(xb.to(device)).argmax(1).cpu()\n",
    "        y_true += yb.tolist(); y_pred += preds.tolist()\n",
    "        rows   += list(zip(fn, le.inverse_transform(preds.numpy())))\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"âœ… Exp3-{tag}  Acc={acc:.4f}  Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 11. ê²°ê³¼ ì €ì¥\n",
    "json.dump({\"experiment\":f\"exp3_{tag}\",\n",
    "           \"sigma\": SIGMA,\n",
    "           \"accuracy\":acc,\n",
    "           \"macro_f1\":macro_f1},\n",
    "          open(OUT_MET, \"w\"), indent=2)\n",
    "\n",
    "pd.DataFrame(rows, columns=[\"filename\",\"predicted_label\"])\\\n",
    "  .to_csv(OUT_PRED, index=False)\n",
    "\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0baa1260-d388-467c-a32d-ad013d839bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.6295\n",
      "[2/10] Loss=0.1813\n",
      "[3/10] Loss=0.1718\n",
      "[4/10] Loss=0.1702\n",
      "[5/10] Loss=0.1700\n",
      "[6/10] Loss=0.1698\n",
      "[7/10] Loss=0.1697\n",
      "[8/10] Loss=0.1697\n",
      "[9/10] Loss=0.1696\n",
      "[10/10] Loss=0.1696\n",
      "âœ… Exp3-noise003  Acc=0.4667  Macro-F1=0.4776\n",
      "ğŸ“„ Metrics â†’ result_exp3_noise003.json\n",
      "ğŸ“„ Preds   â†’ pred_exp3_noise003.csv\n",
      "ğŸ–¼ï¸  Saved 6 augmented images â†’ ./aug_vis_noise003\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp3_noise_9x.py â€• ì‹¤í—˜êµ° 3\n",
    "  Â· Train : 70 ì›ë³¸ Ã— 9 ë°° (Flip + ColorJitter + GaussianNoise Ïƒ=0.03) = 630\n",
    "  Â· Test  : split42_70-30.json ì˜ 30 ì›ë³¸\n",
    "  Â· Aug PNG ì €ì¥: ì• 2 ë°°ì¹˜ / í´ë˜ìŠ¤ë‹¹ 2 ì¥ / ì´ 120 ì¥ í•œë„\n",
    "ì¶œë ¥ : result_exp3_noise003.json , pred_exp3_noise003.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import\n",
    "import os, json, random, math, argparse, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ì¸ì & ê³ ì •ê°’\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--sigma\", type=float, default=0.03,\n",
    "                    help=\"Gaussian noise std (default 0.03)\")\n",
    "args, _ = parser.parse_known_args()          # Jupyter -f ì˜µì…˜ ë¬´ì‹œ\n",
    "SIGMA = args.sigma                           # 0.03\n",
    "TAG   = f\"noise{SIGMA:.2f}\".replace('.', '') # noise003\n",
    "\n",
    "SEED = 42\n",
    "CSV  = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG  = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "SPLIT = \"split42_70-30.json\"\n",
    "\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "\n",
    "OUT_MET  = f\"result_exp3_{TAG}.json\"\n",
    "OUT_PRED = f\"pred_exp3_{TAG}.csv\"\n",
    "\n",
    "# --- ì¦ê°•ë³¸ ì €ì¥ ì˜µì…˜ ---\n",
    "SAVE_AUG            = True\n",
    "SAVE_DIR            = f\"./aug_vis_{TAG}\"\n",
    "SAVE_FIRST_BATCHES  = 2     # (a) ì²˜ìŒ k ë°°ì¹˜\n",
    "SAVE_PER_CLASS      = 2     # (b) í´ë˜ìŠ¤ë³„ ìµœëŒ€ c ì¥\n",
    "SAVE_TOTAL_LIMIT    = 120   # (c) ì „ì²´ n ì¥\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ì‹œë“œ ê³ ì •\n",
    "def seed_all(s: int):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Dataset ë° ë³€í™˜\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, std: float):\n",
    "        self.std = std\n",
    "    def __call__(self, t: torch.Tensor):\n",
    "        return t + torch.randn_like(t) * self.std\n",
    "    def __repr__(self):\n",
    "        return f\"AddGaussianNoise(std={self.std})\"\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(SIGMA),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tf, le, save_aug=False):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.dir = IMG\n",
    "        self.tf = tf\n",
    "        self.cls = le.transform(self.df[\"weight_class\"])\n",
    "        self.save_aug = save_aug\n",
    "        self.saved_total = 0\n",
    "        self.class_cnt = {i: 0 for i in range(len(le.classes_))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_pil = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        tensor = self.tf(img_pil)\n",
    "\n",
    "        # ---- ì¦ê°•ë³¸ ì €ì¥ ----\n",
    "        if (self.save_aug\n",
    "            and self.saved_total < SAVE_TOTAL_LIMIT\n",
    "            and self.saved_total < SAVE_FIRST_BATCHES * BATCH\n",
    "            and self.class_cnt[self.cls[idx]] < SAVE_PER_CLASS):\n",
    "            fname = f\"{self.saved_total:04d}_{row.filename}\"\n",
    "            save_path = os.path.join(SAVE_DIR, fname)\n",
    "            save_image((tensor * 0.5 + 0.5).clamp(0, 1), save_path)\n",
    "            self.saved_total += 1\n",
    "            self.class_cnt[self.cls[idx]] += 1\n",
    "        # ---------------------\n",
    "\n",
    "        return tensor, torch.tensor(self.cls[idx]), row.filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë°ì´í„° ë¶„í•  ë¡œë“œ\n",
    "df = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx = json.load(open(SPLIT))\n",
    "    train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), train_size=70, test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\": train_idx, \"test\": test_idx}, open(SPLIT, \"w\"))\n",
    "\n",
    "train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. DataLoader (9Ã—)\n",
    "base_ds = ScrapDataset(train_df, train_tf, le, save_aug=SAVE_AUG)\n",
    "train_ds = ConcatDataset([base_ds] * 9)  # 630\n",
    "test_ds = ScrapDataset(test_df, test_tf, le, save_aug=False)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "test_ld = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ëª¨ë¸ Â· Optim Â· Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls: int):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium', pretrained=True, num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoaTMedium(len(le.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps = len(train_ld) * EPOCHS\n",
    "warmup_steps = len(train_ld)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / warmup_steps\n",
    "    prog = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * prog))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. í•™ìŠµ\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb, _ in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={epoch_loss/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. í‰ê°€\n",
    "model.eval()\n",
    "yt, yp, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, fn in test_ld:\n",
    "        preds = model(xb.to(device)).argmax(1).cpu()\n",
    "        yt += yb.tolist(); yp += preds.tolist()\n",
    "        rows += list(zip(fn, le.inverse_transform(preds.numpy())))\n",
    "acc = accuracy_score(yt, yp)\n",
    "f1  = f1_score(yt, yp, average=\"macro\")\n",
    "print(f\"âœ… Exp3-{TAG}  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. ê²°ê³¼ ì €ì¥\n",
    "json.dump({\"experiment\": f\"exp3_{TAG}\", \"sigma\": SIGMA,\n",
    "           \"accuracy\": acc, \"macro_f1\": f1},\n",
    "          open(OUT_MET, \"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\", \"predicted_label\"]).to_csv(OUT_PRED, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n",
    "\n",
    "if SAVE_AUG:\n",
    "    print(f\"ğŸ–¼ï¸  Saved {base_ds.saved_total} augmented images â†’ {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e2a8bd-249e-4694-80e8-97490d523ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.6491\n",
      "[2/10] Loss=0.1828\n",
      "[3/10] Loss=0.1720\n",
      "[4/10] Loss=0.1707\n",
      "[5/10] Loss=0.1703\n",
      "[6/10] Loss=0.1701\n",
      "[7/10] Loss=0.1700\n",
      "[8/10] Loss=0.1700\n",
      "[9/10] Loss=0.1699\n",
      "[10/10] Loss=0.1699\n",
      "âœ… Exp3-noise010  Acc=0.5000  Macro-F1=0.5068\n",
      "ğŸ“„ Metrics â†’ result_exp3_noise010.json\n",
      "ğŸ“„ Preds   â†’ pred_exp3_noise010.csv\n",
      "ğŸ–¼ï¸  Saved 6 augmented images â†’ ./aug_vis_noise010\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp3_noise_9x.py â€• ì‹¤í—˜êµ° 3\n",
    "  Â· Train : 70 ì›ë³¸ Ã— 9 ë°° (Flip + ColorJitter + GaussianNoise Ïƒ=0.1) = 630\n",
    "  Â· Test  : split42_70-30.json ì˜ 30 ì›ë³¸\n",
    "  Â· Aug PNG ì €ì¥: ì• 2 ë°°ì¹˜ / í´ë˜ìŠ¤ë‹¹ 2 ì¥ / ì´ 120 ì¥ í•œë„\n",
    "ì¶œë ¥ : result_exp3_noise003.json , pred_exp3_noise003.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import\n",
    "import os, json, random, math, argparse, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ì¸ì & ê³ ì •ê°’\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--sigma\", type=float, default=0.1,\n",
    "                    help=\"Gaussian noise std (default 0.1)\")\n",
    "args, _ = parser.parse_known_args()          # Jupyter -f ì˜µì…˜ ë¬´ì‹œ\n",
    "SIGMA = args.sigma                           \n",
    "TAG   = f\"noise{SIGMA:.2f}\".replace('.', '') \n",
    "\n",
    "SEED = 42\n",
    "CSV  = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG  = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "SPLIT = \"split42_70-30.json\"\n",
    "\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "\n",
    "OUT_MET  = f\"result_exp3_{TAG}.json\"\n",
    "OUT_PRED = f\"pred_exp3_{TAG}.csv\"\n",
    "\n",
    "# --- ì¦ê°•ë³¸ ì €ì¥ ì˜µì…˜ ---\n",
    "SAVE_AUG            = True\n",
    "SAVE_DIR            = f\"./aug_vis_{TAG}\"\n",
    "SAVE_FIRST_BATCHES  = 2     # (a) ì²˜ìŒ k ë°°ì¹˜\n",
    "SAVE_PER_CLASS      = 2     # (b) í´ë˜ìŠ¤ë³„ ìµœëŒ€ c ì¥\n",
    "SAVE_TOTAL_LIMIT    = 120   # (c) ì „ì²´ n ì¥\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ì‹œë“œ ê³ ì •\n",
    "def seed_all(s: int):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Dataset ë° ë³€í™˜\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, std: float):\n",
    "        self.std = std\n",
    "    def __call__(self, t: torch.Tensor):\n",
    "        return t + torch.randn_like(t) * self.std\n",
    "    def __repr__(self):\n",
    "        return f\"AddGaussianNoise(std={self.std})\"\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(SIGMA),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tf, le, save_aug=False):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.dir = IMG\n",
    "        self.tf = tf\n",
    "        self.cls = le.transform(self.df[\"weight_class\"])\n",
    "        self.save_aug = save_aug\n",
    "        self.saved_total = 0\n",
    "        self.class_cnt = {i: 0 for i in range(len(le.classes_))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_pil = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        tensor = self.tf(img_pil)\n",
    "\n",
    "        # ---- ì¦ê°•ë³¸ ì €ì¥ ----\n",
    "        if (self.save_aug\n",
    "            and self.saved_total < SAVE_TOTAL_LIMIT\n",
    "            and self.saved_total < SAVE_FIRST_BATCHES * BATCH\n",
    "            and self.class_cnt[self.cls[idx]] < SAVE_PER_CLASS):\n",
    "            fname = f\"{self.saved_total:04d}_{row.filename}\"\n",
    "            save_path = os.path.join(SAVE_DIR, fname)\n",
    "            save_image((tensor * 0.5 + 0.5).clamp(0, 1), save_path)\n",
    "            self.saved_total += 1\n",
    "            self.class_cnt[self.cls[idx]] += 1\n",
    "        # ---------------------\n",
    "\n",
    "        return tensor, torch.tensor(self.cls[idx]), row.filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë°ì´í„° ë¶„í•  ë¡œë“œ\n",
    "df = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx = json.load(open(SPLIT))\n",
    "    train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), train_size=70, test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\": train_idx, \"test\": test_idx}, open(SPLIT, \"w\"))\n",
    "\n",
    "train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. DataLoader (9Ã—)\n",
    "base_ds = ScrapDataset(train_df, train_tf, le, save_aug=SAVE_AUG)\n",
    "train_ds = ConcatDataset([base_ds] * 9)  # 630\n",
    "test_ds = ScrapDataset(test_df, test_tf, le, save_aug=False)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "test_ld = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ëª¨ë¸ Â· Optim Â· Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls: int):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium', pretrained=True, num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoaTMedium(len(le.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps = len(train_ld) * EPOCHS\n",
    "warmup_steps = len(train_ld)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / warmup_steps\n",
    "    prog = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * prog))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. í•™ìŠµ\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb, _ in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={epoch_loss/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. í‰ê°€\n",
    "model.eval()\n",
    "yt, yp, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, fn in test_ld:\n",
    "        preds = model(xb.to(device)).argmax(1).cpu()\n",
    "        yt += yb.tolist(); yp += preds.tolist()\n",
    "        rows += list(zip(fn, le.inverse_transform(preds.numpy())))\n",
    "acc = accuracy_score(yt, yp)\n",
    "f1  = f1_score(yt, yp, average=\"macro\")\n",
    "print(f\"âœ… Exp3-{TAG}  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. ê²°ê³¼ ì €ì¥\n",
    "json.dump({\"experiment\": f\"exp3_{TAG}\", \"sigma\": SIGMA,\n",
    "           \"accuracy\": acc, \"macro_f1\": f1},\n",
    "          open(OUT_MET, \"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\", \"predicted_label\"]).to_csv(OUT_PRED, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n",
    "\n",
    "if SAVE_AUG:\n",
    "    print(f\"ğŸ–¼ï¸  Saved {base_ds.saved_total} augmented images â†’ {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f3da37-1fc6-4cf2-8bee-128a11426a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.6726\n",
      "[2/10] Loss=0.1984\n",
      "[3/10] Loss=0.1742\n",
      "[4/10] Loss=0.1714\n",
      "[5/10] Loss=0.1709\n",
      "[6/10] Loss=0.1705\n",
      "[7/10] Loss=0.1703\n",
      "[8/10] Loss=0.1701\n",
      "[9/10] Loss=0.1700\n",
      "[10/10] Loss=0.1701\n",
      "âœ… Exp3-poisson127  Acc=0.5333  Macro-F1=0.5345\n",
      "ğŸ“„ Metrics â†’ result_exp3_poisson127.json\n",
      "ğŸ“„ Preds   â†’ pred_exp3_poisson127.csv\n",
      "ğŸ–¼ï¸  Saved 6 augmented images â†’ ./aug_vis_poisson127\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp3_poisson_9x.py â€• ì‹¤í—˜êµ°-3 (Poisson Shot Noise, 9Ã—)\n",
    "  Â· Train : 70 ì›ë³¸ Ã— 9ë°° (Flip + CJ + PoissonNoise) = 630\n",
    "  Â· Test  : split42_70-30.json ì˜ 30 ì›ë³¸\n",
    "  Â· Î»-scale : 127  (í”½ì…€ 0-1 ë²”ìœ„ë¥¼ 0-Î» ë¡œ ë§¤í•‘)\n",
    "  Â· ì¦ê°• PNG ì €ì¥ : ì• 2 ë°°ì¹˜ Ã— í´ë˜ìŠ¤ë‹¹ 2ì¥ Ã— ì´ 120ì¥ í•œë„\n",
    "ì¶œë ¥ : result_exp3_poissonÎ»127.json / pred_exp3_poissonÎ»127.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import\n",
    "import os, json, random, math, argparse, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. CLI\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lambda_scale\", type=float, default=127,\n",
    "                    help=\"Poisson Î» scaling factor (default 127)\")\n",
    "args, _ = parser.parse_known_args()\n",
    "LAM = args.lambda_scale               # shot-noise Î»\n",
    "TAG = f\"poisson{int(LAM)}\"            # e.g. poisson127\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ê³ ì • íŒŒë¼ë¯¸í„°\n",
    "SEED=42\n",
    "CSV = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "SPLIT=\"split42_70-30.json\"\n",
    "\n",
    "BATCH=16; EPOCHS=10; LR=1e-4; WD=3e-4; SMOOTH=0.05\n",
    "OUT_MET = f\"result_exp3_{TAG}.json\"\n",
    "OUT_PRED= f\"pred_exp3_{TAG}.csv\"\n",
    "\n",
    "# ì¦ê°•ë³¸ ì €ì¥ ì˜µì…˜\n",
    "SAVE_AUG=True\n",
    "SAVE_DIR=f\"./aug_vis_{TAG}\"\n",
    "SAVE_FIRST_BATCHES=2; SAVE_PER_CLASS=2; SAVE_TOTAL_LIMIT=120\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ì‹œë“œ\n",
    "def seed_all(s:int):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed_all(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë³€í™˜\n",
    "class AddPoissonNoise:\n",
    "    \"\"\"tensor âˆˆ [-1,1] â†’ Poisson ë…¸ì´ì¦ˆ í›„ ë‹¤ì‹œ [-1,1]\"\"\"\n",
    "    def __init__(self, lam_scale: float = 127):\n",
    "        self.lam = lam_scale\n",
    "    def __call__(self, t: torch.Tensor):\n",
    "        t01 = (t * 0.5 + 0.5).clamp(0,1)            # [0,1]\n",
    "        noisy = torch.poisson(t01 * self.lam) / self.lam\n",
    "        return (noisy * 2 - 1).clamp(-1,1)          # back to [-1,1]\n",
    "    def __repr__(self): return f\"AddPoissonNoise(lam={self.lam})\"\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    AddPoissonNoise(LAM),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Dataset\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, df, tf, le, save_aug=False):\n",
    "        self.df=df.reset_index(drop=True).copy()\n",
    "        self.dir=IMG; self.tf=tf; self.le=le\n",
    "        self.cls = le.transform(self.df[\"weight_class\"])\n",
    "        self.save_aug=save_aug; self.saved_tot=0\n",
    "        self.class_cnt={i:0 for i in range(len(le.classes_))}\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row=self.df.iloc[idx]\n",
    "        pil=Image.open(os.path.join(self.dir,row.filename)).convert(\"RGB\")\n",
    "        tensor=self.tf(pil)\n",
    "        if (self.save_aug and\n",
    "            self.saved_tot < SAVE_TOTAL_LIMIT and\n",
    "            self.saved_tot < SAVE_FIRST_BATCHES*BATCH and\n",
    "            self.class_cnt[self.cls[idx]] < SAVE_PER_CLASS):\n",
    "            fname=f\"{self.saved_tot:04d}_{row.filename}\"\n",
    "            save_image((tensor*0.5+0.5).clamp(0,1), os.path.join(SAVE_DIR,fname))\n",
    "            self.class_cnt[self.cls[idx]]+=1; self.saved_tot+=1\n",
    "        return tensor, torch.tensor(self.cls[idx]), row.filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. split\n",
    "df=pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx=json.load(open(SPLIT)); tr_idx, te_idx=idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    tr_idx, te_idx=train_test_split(range(len(df)),train_size=70,test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":tr_idx,\"test\":te_idx}, open(SPLIT,\"w\"))\n",
    "train_df, test_df = df.iloc[tr_idx], df.iloc[te_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. DataLoader (9Ã—)\n",
    "base_ds=ScrapDataset(train_df, train_tf, le, save_aug=SAVE_AUG)\n",
    "train_ds=ConcatDataset([base_ds]*9)\n",
    "test_ds =ScrapDataset(test_df, test_tf, le)\n",
    "train_ld=DataLoader(train_ds,batch_size=BATCH,shuffle=True ,num_workers=0)\n",
    "test_ld =DataLoader(test_ds ,batch_size=BATCH,shuffle=False,num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. ëª¨ë¸Â·OptimÂ·Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls: int):\n",
    "        super().__init__()\n",
    "        # timm backbone\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium',\n",
    "            pretrained=True,\n",
    "            num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CoaTMedium(len(le.classes_)).to(device)\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps=len(train_ld)*EPOCHS; warmup=len(train_ld)\n",
    "scheduler=torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda s:(s+1)/warmup if s<warmup\n",
    "             else 0.5*(1+math.cos(math.pi*(s-warmup)/(total_steps-warmup))))\n",
    "criterion=nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. í•™ìŠµ\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); loss_sum=0\n",
    "    for xb,yb,_ in train_ld:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(); loss=criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={loss_sum/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10. í‰ê°€\n",
    "model.eval(); yt,yp,rows=[],[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb,f in test_ld:\n",
    "        p=model(xb.to(device)).argmax(1).cpu()\n",
    "        yt+= yb.tolist(); yp+= p.tolist()\n",
    "        rows+=list(zip(f, le.inverse_transform(p.numpy())))\n",
    "acc=accuracy_score(yt,yp); f1=f1_score(yt,yp,average=\"macro\")\n",
    "print(f\"âœ… Exp3-{TAG}  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 11. ì €ì¥\n",
    "json.dump({\"experiment\":f\"exp3_{TAG}\",\"lambda_scale\":LAM,\n",
    "           \"accuracy\":acc,\"macro_f1\":f1},\n",
    "          open(OUT_MET,\"w\"), indent=2)\n",
    "pd.DataFrame(rows,columns=[\"filename\",\"predicted_label\"]).to_csv(OUT_PRED,index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n",
    "if SAVE_AUG:\n",
    "    print(f\"ğŸ–¼ï¸  Saved {base_ds.saved_tot} augmented images â†’ {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd165977-c15a-465d-8de3-b14af86cfe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.6028\n",
      "[2/10] Loss=0.1845\n",
      "[3/10] Loss=0.1718\n",
      "[4/10] Loss=0.1705\n",
      "[5/10] Loss=0.1699\n",
      "[6/10] Loss=0.1698\n",
      "[7/10] Loss=0.1698\n",
      "[8/10] Loss=0.1697\n",
      "[9/10] Loss=0.1696\n",
      "[10/10] Loss=0.1696\n",
      "âœ… Exp3-speckle080  Acc=0.4667  Macro-F1=0.4522\n",
      "ğŸ“„ Metrics â†’ result_exp3_speckle080.json\n",
      "ğŸ“„ Preds   â†’ pred_exp3_speckle080.csv\n",
      "ğŸ–¼ï¸  Saved 6 augmented images â†’ ./aug_vis_speckle080\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp3_speckle_9x.py  â€•  ì‹¤í—˜êµ°-3 (Speckle Noise Ïƒ=0.08, 9Ã—)\n",
    "  â€¢ Train : 70 ì›ë³¸ Ã— 9ë°° (Flip + ColorJitter + Speckle Ïƒ) = 630\n",
    "  â€¢ Test  : split42_70-30.jsonì˜ 30 ì›ë³¸\n",
    "  â€¢ PNG ì €ì¥ : ì• 2ë°°ì¹˜ / í´ë˜ìŠ¤ë‹¹ 2ì¥ / ì´ 120ì¥ í•œë„\n",
    "ì¶œë ¥ : result_exp3_speckle008.json / pred_exp3_speckle008.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import\n",
    "import os, json, random, math, argparse, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. CLI\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--sigma\", type=float, default=0.08,\n",
    "                    help=\"Speckle noise Ïƒ (default 0.08)\")\n",
    "args, _ = parser.parse_known_args()\n",
    "SIGMA = args.sigma                               # 0.08\n",
    "TAG = f\"speckle{int(SIGMA*1000):03d}\"            # speckle008\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ê³ ì • íŒŒë¼ë¯¸í„°\n",
    "SEED = 42\n",
    "CSV  = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG  = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "SPLIT = \"split42_70-30.json\"\n",
    "\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "\n",
    "OUT_MET  = f\"result_exp3_{TAG}.json\"\n",
    "OUT_PRED = f\"pred_exp3_{TAG}.csv\"\n",
    "\n",
    "SAVE_AUG           = True\n",
    "SAVE_FIRST_BATCHES = 2\n",
    "SAVE_PER_CLASS     = 2\n",
    "SAVE_TOTAL_LIMIT   = 120\n",
    "SAVE_DIR           = f\"./aug_vis_{TAG}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ì‹œë“œ ê³ ì •\n",
    "def seed_all(s: int):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all(SEED)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë³€í™˜\n",
    "class AddSpeckleNoise:\n",
    "    \"\"\"I_noisy = I + I * N(0, ÏƒÂ²)\"\"\"\n",
    "    def __init__(self, std: float):\n",
    "        self.std = std\n",
    "    def __call__(self, t: torch.Tensor):\n",
    "        return (t + t * torch.randn_like(t) * self.std).clamp(-1, 1)\n",
    "    def __repr__(self):\n",
    "        return f\"AddSpeckleNoise(std={self.std})\"\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    AddSpeckleNoise(SIGMA),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Dataset\n",
    "class ScrapDS(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tf, le, save_aug=False):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.dir = IMG\n",
    "        self.tf = tf\n",
    "        self.labels = le.transform(self.df[\"weight_class\"])\n",
    "        self.save_aug = save_aug\n",
    "        self.saved_total = 0\n",
    "        self.class_cnt = {i: 0 for i in range(len(le.classes_))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        tensor = self.tf(img)\n",
    "\n",
    "        # ì €ì¥ ë¡œì§\n",
    "        if (\n",
    "            self.save_aug\n",
    "            and self.saved_total < SAVE_TOTAL_LIMIT\n",
    "            and self.saved_total < SAVE_FIRST_BATCHES * BATCH\n",
    "            and self.class_cnt[self.labels[idx]] < SAVE_PER_CLASS\n",
    "        ):\n",
    "            fname = f\"{self.saved_total:04d}_{row.filename}\"\n",
    "            save_image((tensor * 0.5 + 0.5).clamp(0, 1), os.path.join(SAVE_DIR, fname))\n",
    "            self.saved_total += 1\n",
    "            self.class_cnt[self.labels[idx]] += 1\n",
    "\n",
    "        return tensor, torch.tensor(self.labels[idx]), row.filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. split\n",
    "df = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx = json.load(open(SPLIT))\n",
    "    train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), train_size=70, test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED\n",
    "    )\n",
    "    json.dump({\"train\": train_idx, \"test\": test_idx}, open(SPLIT, \"w\"))\n",
    "\n",
    "train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. DataLoader (9Ã—)\n",
    "base_ds = ScrapDS(train_df, train_tf, le, save_aug=SAVE_AUG)\n",
    "train_ds = ConcatDataset([base_ds] * 9)          # 630\n",
    "test_ds  = ScrapDS(test_df, test_tf, le)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0)\n",
    "test_ld  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. ëª¨ë¸ Â· Optim Â· Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls: int):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium', pretrained=True, num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoaTMedium(len(le.classes_)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps = len(train_ld) * EPOCHS\n",
    "warmup_steps = len(train_ld)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / warmup_steps\n",
    "    prog = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "    return 0.5 * (1 + math.cos(math.pi * prog))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. í•™ìŠµ\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb, _ in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={epoch_loss/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10. í‰ê°€\n",
    "model.eval()\n",
    "yt, yp, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, fn in test_ld:\n",
    "        preds = model(xb.to(device)).argmax(1).cpu()\n",
    "        yt += yb.tolist()\n",
    "        yp += preds.tolist()\n",
    "        rows += list(zip(fn, le.inverse_transform(preds.numpy())))\n",
    "\n",
    "acc = accuracy_score(yt, yp)\n",
    "f1  = f1_score(yt, yp, average=\"macro\")\n",
    "print(f\"âœ… Exp3-{TAG}  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 11. ì €ì¥\n",
    "json.dump(\n",
    "    {\n",
    "        \"experiment\": f\"exp3_{TAG}\",\n",
    "        \"sigma\": SIGMA,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "    },\n",
    "    open(OUT_MET, \"w\"),\n",
    "    indent=2,\n",
    ")\n",
    "pd.DataFrame(rows, columns=[\"filename\", \"predicted_label\"]).to_csv(OUT_PRED, index=False)\n",
    "\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n",
    "if SAVE_AUG:\n",
    "    print(f\"ğŸ–¼ï¸  Saved {base_ds.saved_total} augmented images â†’ {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296eaa81-7089-4608-afeb-7b5dbf1bac06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf54f53-9681-450c-b2f3-ec24b852ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ëŒ€ì¡°êµ°) ê¸°ë³¸ í•™ìŠµê°’\n",
    "ì‹¤í—˜êµ°1) ê¸°ì¡´ ìƒí•˜ì¢Œìš° ë°˜ì „ (p=1ì´ë©´ ì•ˆë¨) + ëª…ë„/ëŒ€ë¹„ ì¡°ì •\n",
    "ì‹¤í—˜êµ°2) ë°ì´í„° ì¦ê°• ì•ˆ í•˜ê³  ê·¸ëƒ¥ 9ë°°ë¡œ ë³µì‚¬ë§Œ í•œ ê²½ìš°\n",
    "ì‹¤í—˜êµ°3) ë…¸ì´ì¦ˆê¹Œì§€ ì¶”ê°€í•œ ê²½ìš° (coat_ar_noise.ipybì°¸ì¡°)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0398aec1-3b03-46c7-993f-6828ce08521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Loss=1.1468\n",
      "[2/5] Loss=0.6229\n",
      "[3/5] Loss=0.2321\n",
      "[4/5] Loss=0.0502\n",
      "[5/5] Loss=0.0159\n",
      "âœ… Control-Base  Acc=0.4286  Macro-F1=0.4339\n",
      "ğŸ“„ Metrics â†’ result_control_base.json\n",
      "ğŸ“„ Preds   â†’ pred_control_base.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_control_base.py â€” ëŒ€ì¡°êµ° (ì¦ê°• ì—†ìŒ, ê¸°ë³¸ í•™ìŠµê°’)\n",
    "ì¶œë ¥ : result_control_base.json  /  pred_control_base.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import\n",
    "import os, json, random, warnings\n",
    "import numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° / ê²½ë¡œ \n",
    "SEED = 42\n",
    "CSV_PATH = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG_DIR  = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "BATCH = 8\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "WD = 0.01                     \n",
    "SMOOTH = 0.0\n",
    "\n",
    "OUT_MET = \"result_control_base.json\"\n",
    "OUT_PRED = \"pred_control_base.csv\"\n",
    "SPLIT_JSON = \"split42.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ì‹œë“œ & ë©€í‹°í”„ë¡œì„¸ì‹±\n",
    "def seed_everything(s):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed_everything(SEED)\n",
    "import torch.multiprocessing as mp\n",
    "if mp.get_start_method(allow_none=True) != \"spawn\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Dataset\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, tf, label_enc):\n",
    "        self.df  = df.reset_index(drop=True).copy()\n",
    "        self.dir = img_dir\n",
    "        self.tf  = tf\n",
    "        self.df[\"cls\"] = label_enc.transform(self.df[\"weight_class\"])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        img = self.tf(img)\n",
    "        return img, torch.tensor(row.cls), row.filename\n",
    "\n",
    "plain_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. split ê³ ì •\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if os.path.exists(SPLIT_JSON):\n",
    "    idx = json.load(open(SPLIT_JSON)); train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), test_size=0.2,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":train_idx, \"test\":test_idx}, open(SPLIT_JSON,\"w\"))\n",
    "train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(train_df[\"weight_class\"])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ScrapDataset(train_df, IMG_DIR, plain_tf, le),\n",
    "    batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "test_loader  = DataLoader(\n",
    "    ScrapDataset(test_df,  IMG_DIR, plain_tf, le),\n",
    "    batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ëª¨ë¸\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model('coat_lite_medium',\n",
    "                                     pretrained=True, num_classes=n_cls)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = CoaTMedium(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "criterion  = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. í•™ìŠµ\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); total=0\n",
    "    for xb, yb, _ in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(); loss = criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step()\n",
    "        total += loss.item()*xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={total/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. í‰ê°€\n",
    "model.eval(); yt, yp, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, f in test_loader:\n",
    "        pred = model(xb.to(device)).argmax(1).cpu()\n",
    "        yt += yb.tolist(); yp += pred.tolist()\n",
    "        rows += list(zip(f, le.inverse_transform(pred.numpy())))\n",
    "acc = accuracy_score(yt, yp)\n",
    "f1  = f1_score(yt, yp, average=\"macro\")\n",
    "print(f\"âœ… Control-Base  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. ì €ì¥\n",
    "json.dump({\"experiment\":\"control_base\",\"accuracy\":acc,\"macro_f1\":f1},\n",
    "          open(OUT_MET,\"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\",\"predicted_label\"]).to_csv(OUT_PRED, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd75e95-bccd-473c-93af-ae0ea14817a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=1.1140\n",
      "[2/10] Loss=0.6211\n",
      "[3/10] Loss=0.3182\n",
      "[4/10] Loss=0.2022\n",
      "[5/10] Loss=0.1760\n",
      "[6/10] Loss=0.1735\n",
      "[7/10] Loss=0.1729\n",
      "[8/10] Loss=0.1723\n",
      "[9/10] Loss=0.1719\n",
      "[10/10] Loss=0.1717\n",
      "âœ… Control-EP10-BS16  Acc=0.5238  Macro-F1=0.5235\n",
      "ğŸ“„ Metrics â†’ result_control_ep10_bs16.json\n",
      "ğŸ“„ Preds   â†’ pred_control_ep10_bs16.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_control_base.py â€” ëŒ€ì¡°êµ° (ì¦ê°• ì—†ìŒ, ê¸°ë³¸ í•™ìŠµê°’)\n",
    "ì¶œë ¥ : result_control_base.json  /  pred_control_base.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€ 0. ê¸°ë³¸ import\n",
    "import os, json, random, math, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€ 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° / ê²½ë¡œ\n",
    "SEED = 42\n",
    "CSV  = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG  = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "BATCH  = 16         \n",
    "EPOCHS = 10         \n",
    "LR     = 1e-4\n",
    "WD     = 3e-4       \n",
    "SMOOTH = 0.05        \n",
    "\n",
    "SPLIT  = \"split42.json\"\n",
    "OUT_MET = \"result_control_ep10_bs16.json\"\n",
    "OUT_PRE = \"pred_control_ep10_bs16.csv\"\n",
    "\n",
    "# â”€â”€â”€ 2. ì‹œë“œ & ë©€í‹°í”„ë¡œì„¸ì‹±\n",
    "def seed_all(s):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed_all(SEED)\n",
    "import torch.multiprocessing as mp\n",
    "if mp.get_start_method(allow_none=True) != \"spawn\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# â”€â”€â”€ 3. Dataset\n",
    "class ScrapDS(Dataset):\n",
    "    def __init__(self, df, tf, le):\n",
    "        self.df=df.reset_index(drop=True).copy()\n",
    "        self.dir=IMG; self.tf=tf\n",
    "        self.df[\"cls\"]=le.transform(self.df[\"weight_class\"])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self,i):\n",
    "        row=self.df.iloc[i]\n",
    "        img=Image.open(os.path.join(self.dir,row.filename)).convert(\"RGB\")\n",
    "        return self.tf(img), torch.tensor(row.cls), row.filename\n",
    "\n",
    "plain_tf=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3,[0.5]*3)\n",
    "])\n",
    "\n",
    "# â”€â”€â”€ 4. split ê³ ì •\n",
    "df=pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx=json.load(open(SPLIT)); train_idx, test_idx=idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx=train_test_split(range(len(df)), test_size=0.2,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":train_idx,\"test\":test_idx}, open(SPLIT,\"w\"))\n",
    "\n",
    "tr_df, te_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(tr_df[\"weight_class\"])\n",
    "\n",
    "train_ld = DataLoader(ScrapDS(tr_df, plain_tf, le), batch_size=BATCH,\n",
    "                      shuffle=True , num_workers=0)\n",
    "test_ld  = DataLoader(ScrapDS(te_df, plain_tf, le), batch_size=BATCH,\n",
    "                      shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€ 5. ëª¨ë¸ & Optim & Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        # timm backbone\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium',\n",
    "            pretrained=True,\n",
    "            num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CoaTMedium(len(le.classes_)).to(device)\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps=len(train_ld)*EPOCHS\n",
    "warmup_steps=len(train_ld)       # 1 epoch warm-up\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return (step+1)/warmup_steps\n",
    "    prog=(step-warmup_steps)/(total_steps-warmup_steps)\n",
    "    return 0.5*(1+math.cos(math.pi*prog))\n",
    "scheduler=torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€ 6. í•™ìŠµ\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train(); loss_sum=0\n",
    "    for xb,yb,_ in train_ld:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(); loss=criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={loss_sum/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€ 7. í‰ê°€\n",
    "model.eval(); yt,yp,rows=[],[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb,f in test_ld:\n",
    "        p=model(xb.to(device)).argmax(1).cpu()\n",
    "        yt+=yb.tolist(); yp+=p.tolist()\n",
    "        rows+=list(zip(f, le.inverse_transform(p.numpy())))\n",
    "acc=accuracy_score(yt,yp)\n",
    "f1 = f1_score(yt,yp,average=\"macro\")\n",
    "print(f\"âœ… Control-EP10-BS16  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€ 8. ì €ì¥\n",
    "json.dump({\"experiment\":\"control_ep10_bs16\",\"accuracy\":acc,\"macro_f1\":f1},\n",
    "          open(OUT_MET,\"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\",\"predicted_label\"])\\\n",
    "  .to_csv(OUT_PRE, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aca1c0b-58dc-4221-949a-5dd17fb1f83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.5999\n",
      "[2/10] Loss=0.1821\n",
      "[3/10] Loss=0.1726\n",
      "[4/10] Loss=0.1703\n",
      "[5/10] Loss=0.1698\n",
      "[6/10] Loss=0.1696\n",
      "[7/10] Loss=0.1695\n",
      "[8/10] Loss=0.1695\n",
      "[9/10] Loss=0.1695\n",
      "[10/10] Loss=0.1694\n",
      "âœ… Exp1-FlipColor-9x  Acc=0.4667  Macro-F1=0.4685\n",
      "ğŸ“„ Metrics â†’ result_exp1_flip_color_9x.json\n",
      "ğŸ“„ Preds   â†’ pred_exp1_flip_color_9x.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp1_flip_color_9x.py â€” ì‹¤í—˜êµ° 1\n",
    "  â€¢ Train : 70ê°œ ì›ë³¸ Ã— 9ë°° = 630 ì´ë¯¸ì§€\n",
    "            (Random H/V Flip 0.5 + ColorJitter 0.2/0.2)\n",
    "  â€¢ Test  : ë‚¨ì€ 30ê°œ ì›ë³¸ ê·¸ëŒ€ë¡œ\n",
    "  â€¢ split42_70-30.json ì¬ì‚¬ìš©, seed 42 ê³ ì •\n",
    "ì¶œë ¥ : result_exp1_flip_color_9x.json / pred_exp1_flip_color_9x.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. import & ê³µí†µ ì„¤ì •\n",
    "import os, json, random, math, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° (ëŒ€ì¡°êµ°ê³¼ ë™ì¼)\n",
    "SEED   = 42\n",
    "CSV    = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG    = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "SPLIT  = \"split42_70-30.json\"           \n",
    "OUT_MET= \"result_exp1_flip_color_9x.json\"\n",
    "OUT_PRE= \"pred_exp1_flip_color_9x.csv\"\n",
    "\n",
    "# ì‹œë“œ ê³ ì •\n",
    "def seed_all(s):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed_all(SEED)\n",
    "import torch.multiprocessing as mp\n",
    "if mp.get_start_method(allow_none=True) != \"spawn\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Dataset ì •ì˜\n",
    "class ScrapDS(Dataset):\n",
    "    def __init__(self, df, tf, le):\n",
    "        self.df=df.reset_index(drop=True).copy()\n",
    "        self.dir=IMG; self.tf=tf\n",
    "        self.df[\"cls\"]=le.transform(self.df[\"weight_class\"])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row=self.df.iloc[i]\n",
    "        img=Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        return self.tf(img), torch.tensor(row.cls), row.filename\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Transform\n",
    "aug_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "plain_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. split 70 / 30 ê³ ì •\n",
    "df = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx=json.load(open(SPLIT)); train_idx, test_idx=idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), train_size=70, test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":train_idx, \"test\":test_idx}, open(SPLIT,\"w\"))\n",
    "\n",
    "tr_df, te_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(tr_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Train / Test DataLoader\n",
    "base_train_ds = ScrapDS(tr_df, aug_tf, le)\n",
    "train_ds      = ConcatDataset([base_train_ds]*9)   # 70 Ã— 9 = 630\n",
    "test_ds       = ScrapDS(te_df, plain_tf, le)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True , num_workers=0)\n",
    "test_ld  = DataLoader(test_ds , batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ëª¨ë¸, Optim, Scheduler\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        # timm backbone â”€ ë“¤ì—¬ì“°ê¸° 8ì¹¸(=4ìŠ¤í˜ì´ìŠ¤Ã—2)\n",
    "        self.net = timm.create_model(\n",
    "            'coat_lite_medium',\n",
    "            pretrained=True,\n",
    "            num_classes=n_cls\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoaTMedium(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps=len(train_ld)*EPOCHS\n",
    "warmup_steps=len(train_ld)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda step: (step+1)/warmup_steps if step<warmup_steps\n",
    "                 else 0.5*(1+math.cos(math.pi*(step-warmup_steps)/(total_steps-warmup_steps)))\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. í•™ìŠµ\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    model.train(); loss_sum=0\n",
    "    for xb,yb,_ in train_ld:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(); loss=criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        loss_sum += loss.item()*xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={loss_sum/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. í‰ê°€\n",
    "model.eval(); yt,yp,rows=[],[],[]\n",
    "with torch.no_grad():\n",
    "    for xb,yb,f in test_ld:\n",
    "        p=model(xb.to(device)).argmax(1).cpu()\n",
    "        yt+= yb.tolist(); yp+= p.tolist()\n",
    "        rows+=list(zip(f, le.inverse_transform(p.numpy())))\n",
    "acc = accuracy_score(yt,yp)\n",
    "f1  = f1_score(yt,yp,average=\"macro\")\n",
    "print(f\"âœ… Exp1-FlipColor-9x  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. ì €ì¥\n",
    "json.dump({\"experiment\":\"exp1_flip_color_9x\",\"accuracy\":acc,\"macro_f1\":f1},\n",
    "          open(OUT_MET,\"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\",\"predicted_label\"])\\\n",
    "  .to_csv(OUT_PRE, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8922b26a-300f-4e73-8748-f0b8984faa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Loss=0.5019\n",
      "[2/10] Loss=0.1720\n",
      "[3/10] Loss=0.1695\n",
      "[4/10] Loss=0.1693\n",
      "[5/10] Loss=0.1693\n",
      "[6/10] Loss=0.1693\n",
      "[7/10] Loss=0.1692\n",
      "[8/10] Loss=0.1692\n",
      "[9/10] Loss=0.1692\n",
      "[10/10] Loss=0.1692\n",
      "âœ… Exp2-Copy9x  Acc=0.5000  Macro-F1=0.5018\n",
      "ğŸ“„ Metrics â†’ result_exp2_copy_9x.json\n",
      "ğŸ“„ Preds   â†’ pred_exp2_copy_9x.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "run_exp2_copy_9x.py â€” ì‹¤í—˜êµ° 2\n",
    "  Â· Train : 70ê°œ ì›ë³¸ Ã— 9ë°° ë³µì œ = 630ì¥ (ì¦ê°• ì—†ìŒ)\n",
    "  Â· Test  : split42_70-30.json ì— ë‚¨ì€ 30ì¥\n",
    "  Â· Hyper : BATCH 16 Â· EPOCHS 10 Â· lr 1e-4 Â· wd 3e-4 Â· smoothing 0.05\n",
    "ì¶œë ¥ : result_exp2_copy_9x.json / pred_exp2_copy_9x.csv\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ 0. ê¸°ë³¸ import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, random, math, warnings, numpy as np, pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# â”€â”€ 1. ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED   = 42\n",
    "CSV    = r\"C:\\Users\\ast\\Documents\\project\\train.csv\"\n",
    "IMG    = r\"C:\\Users\\ast\\Documents\\project\\train_images\"\n",
    "BATCH  = 16\n",
    "EPOCHS = 10\n",
    "LR     = 1e-4\n",
    "WD     = 3e-4\n",
    "SMOOTH = 0.05\n",
    "\n",
    "SPLIT  = \"split42_70-30.json\"\n",
    "OUT_MET= \"result_exp2_copy_9x.json\"\n",
    "OUT_PRE= \"pred_exp2_copy_9x.csv\"\n",
    "\n",
    "# â”€â”€ 2. ì‹œë“œ & ë©€í‹°í”„ë¡œì„¸ì‹± ê³ ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def seed_all(s):\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "seed_all(SEED)\n",
    "import torch.multiprocessing as mp\n",
    "if mp.get_start_method(allow_none=True) != \"spawn\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# â”€â”€ 3. Dataset ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ScrapDataset(Dataset):\n",
    "    def __init__(self, df, tf, le):\n",
    "        self.df  = df.reset_index(drop=True).copy()\n",
    "        self.dir = IMG\n",
    "        self.tf  = tf\n",
    "        self.df[\"cls\"] = le.transform(self.df[\"weight_class\"])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.dir, row.filename)).convert(\"RGB\")\n",
    "        return self.tf(img), torch.tensor(row.cls), row.filename\n",
    "\n",
    "plain_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# â”€â”€ 4. 70/30 split ê³ ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV)\n",
    "if os.path.exists(SPLIT):\n",
    "    idx = json.load(open(SPLIT)); train_idx, test_idx = idx[\"train\"], idx[\"test\"]\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(df)), train_size=70, test_size=30,\n",
    "        stratify=df[\"weight_class\"], random_state=SEED)\n",
    "    json.dump({\"train\":train_idx, \"test\":test_idx}, open(SPLIT,\"w\"))\n",
    "\n",
    "tr_df, te_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "le = LabelEncoder().fit(tr_df[\"weight_class\"])\n",
    "\n",
    "# â”€â”€ 5. DataLoader (ì›ë³¸ 9Ã— ë³µì œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "base_train_ds = ScrapDataset(tr_df, plain_tf, le)       # ì¦ê°• X\n",
    "train_ds      = ConcatDataset([base_train_ds]*9)        # 70 Ã— 9 = 630\n",
    "test_ds       = ScrapDataset(te_df, plain_tf, le)\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=BATCH, shuffle=True , num_workers=0)\n",
    "test_ld  = DataLoader(test_ds , batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# â”€â”€ 6. ëª¨ë¸ & Optim & ìŠ¤ì¼€ì¤„ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CoaTMedium(nn.Module):\n",
    "    def __init__(self, n_cls):\n",
    "        super().__init__()\n",
    "        self.net = timm.create_model('coat_lite_medium',\n",
    "                                     pretrained=True, num_classes=n_cls)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = CoaTMedium(len(le.classes_)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "total_steps = len(train_ld) * EPOCHS\n",
    "warmup_steps = len(train_ld)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: (step+1)/warmup_steps if step < warmup_steps\n",
    "             else 0.5*(1+math.cos(math.pi*(step-warmup_steps)/(total_steps-warmup_steps)))\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=SMOOTH)\n",
    "\n",
    "# â”€â”€ 7. í•™ìŠµ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); epoch_loss = 0.0\n",
    "    for xb, yb, _ in train_ld:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    print(f\"[{ep}/{EPOCHS}] Loss={epoch_loss/len(train_ld.dataset):.4f}\")\n",
    "\n",
    "# â”€â”€ 8. í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.eval(); yt, yp, rows = [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb, f in test_ld:\n",
    "        preds = model(xb.to(device)).argmax(1).cpu()\n",
    "        yt += yb.tolist(); yp += preds.tolist()\n",
    "        rows += list(zip(f, le.inverse_transform(preds.numpy())))\n",
    "acc = accuracy_score(yt, yp)\n",
    "f1  = f1_score(yt, yp, average=\"macro\")\n",
    "print(f\"âœ… Exp2-Copy9x  Acc={acc:.4f}  Macro-F1={f1:.4f}\")\n",
    "\n",
    "# â”€â”€ 9. ê²°ê³¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "json.dump({\"experiment\":\"exp2_copy_9x\",\"accuracy\":acc,\"macro_f1\":f1},\n",
    "          open(OUT_MET,\"w\"), indent=2)\n",
    "pd.DataFrame(rows, columns=[\"filename\",\"predicted_label\"])\\\n",
    "  .to_csv(OUT_PRE, index=False)\n",
    "print(f\"ğŸ“„ Metrics â†’ {OUT_MET}\\nğŸ“„ Preds   â†’ {OUT_PRE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
